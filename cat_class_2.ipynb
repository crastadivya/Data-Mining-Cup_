{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('./data/training_data.csv', delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the column names dictionary from the CSV file into a DataFrame\n",
    "col_dict_df = pd.read_csv('data/column_names_dictionary.csv', delimiter=';')\n",
    "col_dict = dict(zip(col_dict_df['CODE'], col_dict_df['INDICATOR NAME']))\n",
    "\n",
    "col_dict[\"I1\"]\n",
    "\n",
    "# Load the group dictionary from the CSV file into a DataFrame\n",
    "group_dict_df = pd.read_csv('data/group_dictionary.csv', delimiter=';')\n",
    "group_dict = dict(zip(group_dict_df['CODE'], group_dict_df['SECTOR']))\n",
    "group_dict[\"G1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns except the first one to numeric\n",
    "for column in data.columns[1:]:\n",
    "    if data[column].dtype == 'object':\n",
    "        data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns except the first one to numeric\n",
    "for column in data.columns[1:]:\n",
    "    if data[column].dtype == 'object':\n",
    "        data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/test_data_no_target.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns except the first one to numeric\n",
    "for column in test_data.columns[1:]:\n",
    "    if test_data[column].dtype == 'object':\n",
    "        test_data[column] = test_data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = data.columns[1:-2]\n",
    "categorical_col = data.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I21, 48,50 and dI21, dI48, dI50 have very high missing values. Also 100% for G1 and G1\n",
    "# so I drop these columns\n",
    "\n",
    "# Also I2 is highly correlated with I3, I8 . I33 is highly correlated with I34.\n",
    "# So I also drop I3, I8, I34\n",
    "\n",
    "data_dropped = data.drop(columns = ['I21','I48','I50','dI21','dI48','dI50','I3','I8','I34'])\n",
    "\n",
    "data_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# impute median by group for the rows with missing values.\n",
    "dfs = []\n",
    "\n",
    "for group_df in data_dropped.groupby('Group'):\n",
    "    group_df = group_df[1]\n",
    "    for col in group_df.columns[1:-2]:\n",
    "        group_df.loc[group_df[col].isna() , col] = group_df[col].median()\n",
    "    dfs.append(group_df)\n",
    "    \n",
    "data_imputed = pd.concat(dfs)\n",
    "data_imputed.head()\n",
    "\n",
    "#data_imputed has imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = data_imputed.isnull()\n",
    "null_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I21, 48,50 and dI21, dI48, dI50 have very high missing values. Also 100% for G1 and G1\n",
    "# so I drop these columns\n",
    "\n",
    "# Also I2 is highly correlated with I3, I8 . I33 is highly correlated with I34.\n",
    "# So I also drop I3, I8, I34\n",
    "\n",
    "test_data_dropped = test_data.drop(columns = ['I21','I48','I50','dI21','dI48','dI50','I3','I8','I34'])\n",
    "\n",
    "test_data_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute median by group for the rows with missing values.\n",
    "dfs = []\n",
    "\n",
    "for group_df in test_data_dropped.groupby('Group'):\n",
    "    group_df = group_df[1]\n",
    "    for col in group_df.columns[1:-2]:\n",
    "        group_df.loc[group_df[col].isna() , col] = group_df[col].median()\n",
    "    dfs.append(group_df)\n",
    "    \n",
    "test_data_imputed = pd.concat(dfs)\n",
    "test_data_imputed.head()\n",
    "\n",
    "#data_imputed has imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = test_data_imputed.isnull()\n",
    "null_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_imputed.iloc[:,:-2]\n",
    "y = data_imputed.loc[:,['Class','Perform']]\n",
    "\n",
    "class_labels = [-1, 0, 1]  # Define the class labels present in your dataset\n",
    "\n",
    "y_shifted = np.array(y['Class']) + 1\n",
    "\n",
    "# Compute the class frequencies\n",
    "class_counts = np.bincount(y_shifted)\n",
    "\n",
    "# Compute the inverse class frequencies\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "class_weights_array = np.zeros(len(data))\n",
    "class_weights_array[data[data['Class'] == -1].index] = class_weights[0]\n",
    "class_weights_array[data[data['Class'] == 0].index] = class_weights[1]\n",
    "class_weights_array[data[data['Class'] == 1].index] = class_weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing done - removed highly correlated, removed NA, imputed group median, class weights.\n",
    "#catboost does'nt need one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Assuming X contains the features and y contains Perform and Class\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Custom prediction logic\n",
    "\n",
    "def custom_predict_proba(probas): # probas is probably a 8000x3 matrix\n",
    "    predictions = []\n",
    "    \n",
    "    for p in probas:\n",
    "        if p[0] >= 0.5:  # p[0] is the probability for class -1\n",
    "            predictions.append(-1)\n",
    "        elif p[2] >= 0.5:  # p[2] is the probability for class 1\n",
    "            predictions.append(1)\n",
    "        else:  # p[1] is the probability for class 0\n",
    "            predictions.append(0)\n",
    "            \n",
    "    return np.array(predictions) # predictions = [-1,1,1,0,...]\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_scorer(catboost_model, X, y):\n",
    "    probas = catboost_model.predict_proba(X)\n",
    "    predictions = custom_predict_proba(probas)\n",
    "    cost_matrix = np.array([[0, 1, 2], [1, 0, 1], [2, 1, 0]])\n",
    "    conf_matrix = np.zeros((3, 3))\n",
    "\n",
    "    for true, pred in zip(y, predictions):\n",
    "        conf_matrix[int(true) + 1][int(pred) + 1] += 1\n",
    "\n",
    "    error_cost = np.sum(conf_matrix * cost_matrix) / len(y)\n",
    "    return -error_cost  # Negative because higher is better for RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Define the CatBoost model\n",
    "catboost_model = CatBoostClassifier(\n",
    "    od_type='Iter',\n",
    "    od_wait=20,\n",
    "    cat_features = ['Group'], \n",
    "    one_hot_max_size = 11\n",
    ")\n",
    "#(od_type and od_wait are for overfitting detection)\n",
    "\n",
    "\n",
    "# Define hyperparameters grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'iterations': [100, 200, 300]\n",
    "    # Add more hyperparameters as needed\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(catboost_model, param_grid, scoring=custom_scorer, cv=5, \n",
    "                           verbose=1, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Fit the model with hyperparameter tuning and cross-validation\n",
    "grid_search.fit(X, y['Class'], sample_weight=class_weights_array)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
